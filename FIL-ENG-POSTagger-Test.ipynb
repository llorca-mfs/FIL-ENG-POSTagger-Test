{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8b4b7d",
   "metadata": {},
   "source": [
    "# Testing Different Monolingual Filipino and English Part of Speech (POS) Taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf8f173",
   "metadata": {},
   "source": [
    "Import FilWordNet Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345c408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "filword_corpus = pd.read_csv(\"processed_corpus_oct_2022.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be099d0",
   "metadata": {},
   "source": [
    "Generate random string from FilWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "361ea3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = filword_corpus[filword_corpus.source_type.isin(['online_forums', 'social_media', 'news_sites'])]\n",
    "\n",
    "#sentences = sentences.loc[~sentences['text'].str.contains('XX_\\w{1,}')]\n",
    "#sentences = sentences.loc[~sentences['text'].str.contains('[^\\x20-\\x7E]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55807252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinatamad pa ako magpalit ng layout\n"
     ]
    }
   ],
   "source": [
    "\n",
    "randInd = random.randrange(len(sentences))\n",
    "filword_randtext = sentences.text[randInd]\n",
    "\n",
    "print(filword_randtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37982c1c",
   "metadata": {},
   "source": [
    "## ENGPOSTs Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afe353",
   "metadata": {},
   "source": [
    "### spaCy Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f79eda",
   "metadata": {},
   "source": [
    "Import spaCy and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc3858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d06aa3",
   "metadata": {},
   "source": [
    "Generate POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6fa3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_spacy(sentence):\n",
    "    \n",
    "    doc = spacy_nlp(sentence)\n",
    "    \n",
    "    for token in doc:\n",
    "        print(token, \": \", token.pos_, \": \", spacy.explain(token.pos_))\n",
    "\n",
    "print_spacy(filword_randtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b296133",
   "metadata": {},
   "source": [
    "### Flair Testing (with FW tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22542e6",
   "metadata": {},
   "source": [
    "Import Flair and tagger to use (pos-english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/pos-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b76547",
   "metadata": {},
   "source": [
    "Generate POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9689d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make example sentence\n",
    "sentence = Sentence(filword_randtext)\n",
    "\n",
    "# predict NER tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence\n",
    "print(sentence)\n",
    "\n",
    "# print predicted NER spans\n",
    "print('The following NER tags are found:')\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('pos'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53130f",
   "metadata": {},
   "source": [
    "### NLTK Testing (default ENGPOST, with FW tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55fac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# [IMPORTANT] if this is your first time running this Python Notebook, run this:\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = word_tokenize(filword_randtext)\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a41663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0419175",
   "metadata": {},
   "source": [
    "## FILPOSTs Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73360371",
   "metadata": {},
   "source": [
    "### FSPOST (Go & Nocon, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af9dcf",
   "metadata": {},
   "source": [
    "Use FSPOST pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fee2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "# These are Windows formatted directories\n",
    "#model = 'model//filipino-left5words-owlqn2-distsim-pref6-inf2.tagger'\n",
    "#jar = 'lib//stanford-postagger.jar'\n",
    "\n",
    "# These are Linux formatted directories\n",
    "model = 'model/filipino-left5words-owlqn2-distsim-pref6-inf2.tagger'\n",
    "jar = 'lib/stanford-postagger.jar'\n",
    "\n",
    "fspost = StanfordPOSTagger(model, path_to_jar=jar)  # Load Tagger Model\n",
    "fspost._SEPARATOR = '|'  # Set separator for proper tuple formatting (word, tag)\n",
    "\n",
    "def set_java_path(file_path):\n",
    "    \"\"\"\n",
    "    Function for setting java path to make Stanford POS Tagger work. Makes use of the 'os' library. Input \"\" to use\n",
    "    default java path, otherwise set the location.\n",
    "    Args:\n",
    "        file_path (str): The java file path / location.\n",
    "    \"\"\"\n",
    "    if file_path == \"\":\n",
    "        java_path = \"C:/Program Files/Java/jdk1.8.0_111/bin/java.exe\"\n",
    "        print(\"Java path set by default\")\n",
    "    else:\n",
    "        java_path = file_path\n",
    "        print(\"Java path set from given\")\n",
    "    os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "def tag_string(sentence):\n",
    "    \"\"\"\n",
    "    Function for tagging a sentence/string. Output is a (word, pos) tuple. To output a POS-only string, enclose this\n",
    "    function with 'format_pos' function. Ex. fspost.format_pos(fspost.tag_string('this is a string')). Same goes for\n",
    "    Stanford's word|tag notation, use 'format_stanford' function.\n",
    "    Args:\n",
    "        sentence (str): The string to be tagged.\n",
    "    Returns:\n",
    "        tagged_string: a list of string tokens containing POS labeled (word, pos) tuples.\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()  # Tokenize Sentence by whitespaces\n",
    "    # print(tokens)\n",
    "    tagged_string = fspost.tag(tokens)\n",
    "    return tagged_string\n",
    "\n",
    "def tag_string_list(sentence_list):\n",
    "    \"\"\"\n",
    "    Function for tagging a list of sentences. Output is a list of (word, pos) tuple. To output a POS-only string,\n",
    "    enclose the elements in this function with 'format_pos' function. Same goes for Stanford's word|tag notation, use\n",
    "    'format_stanford' function.\n",
    "    Args:\n",
    "        sentence_list (list): The list of strings to be tagged.\n",
    "    Returns:\n",
    "        tagged_list: a list of strings containing POS labelled (word, pos) tuples.\n",
    "    \"\"\"\n",
    "    progress_ctr = 0\n",
    "    tagged_list = []  # Initialize an empty list\n",
    "    for sentence in sentence_list:\n",
    "        tagged_tuple = tag_string(sentence)  # Tag each sentence in the list\n",
    "        tagged_list.append(tagged_tuple)  # Insert tagged sentence in the new list\n",
    "        progress_ctr += 1\n",
    "        print(progress_ctr, \"/\", len(sentence_list))  # Progress Counter\n",
    "    return tagged_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212e2ad",
   "metadata": {},
   "source": [
    "[REQUIRED] Set JDK Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b94167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOWS\n",
    "# set_java_path(\"C:/Program Files/Java/jdk-19/bin/java.exe\")\n",
    "\n",
    "# LINUX\n",
    "set_java_path(\"/usr/lib/jvm/java-11-openjdk-amd64/bin/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638af062",
   "metadata": {},
   "source": [
    "Generate POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_string(filword_randtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a88f445",
   "metadata": {},
   "source": [
    "### LSTM Based Filipino POS Tagger (Cruz, 2020)  ***unfinished***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as datautils\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.utils import predict, normalize, produce_vocab, proc_set, init_weights, accuracy\n",
    "from utils.model import LSTMTagger\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--do_train', action='store_true', help='Train a part of speech tagger.')\n",
    "    parser.add_argument('--do_predict', action='store_true', help='Use a trained model to predict parts of speech.')\n",
    "    parser.add_argument('--seed', type=int, default=1234, help='Random seed.')\n",
    "    parser.add_argument('--checkpoint', type=str, default='checkpoint', help='Location to save model.')\n",
    "    parser.add_argument('--overwrite_save_directory', action='store_true', help='Overwrite the save directory if it exists.')\n",
    "\n",
    "    parser.add_argument('--train_data', type=str, help='Training text dataset.')\n",
    "    parser.add_argument('--evaluation_data', type=str, help='Evaluation text dataset.')\n",
    "    parser.add_argument('--train_tags', type=str, help='Training tags dataset.')\n",
    "    parser.add_argument('--evaluation_tags', type=str, help='Evaluation tags dataset.')\n",
    "    parser.add_argument('--no_cuda', action='store_true', help='Do not use a GPU.')\n",
    "    \n",
    "    parser.add_argument('--embedding_dim', type=int, default=300, help='Embedding dimension.')\n",
    "    parser.add_argument('--num_layers', type=int, default=1, help='Number of recurrent layers.')\n",
    "    parser.add_argument('--bidirectional', action='store_true', help='Use a bidirectional RNN.')\n",
    "    parser.add_argument('--hidden_dim', type=int, default=512, help='Hidden dimension.')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5, help='Dropout probability.')\n",
    "    parser.add_argument('--recur_dropout', type=float, default=0.1, help='Recurrent dropout probability.')\n",
    "    parser.add_argument('--min_freq', type=int, default=1, help='Minimum frequency of words to be added to vocabulary.')\n",
    "    parser.add_argument('--msl', type=int, default=128, help='Maximum sequence length of text.')\n",
    "    parser.add_argument('--bs', type=int, default=128, help='Batch size.')\n",
    "    parser.add_argument('--learning_rate', type=float, default=3e-4, help='Learning rate.')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay.')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs to train.')\n",
    "    parser.add_argument('--sentence', type=str, default='Hello', help='Sentence to predict')\n",
    "    '''\n",
    "    \n",
    "    # args = parser.parse_args()\n",
    "    torch.manual_seed(args.seed);\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    '''\n",
    "    if args.do_train:\n",
    "        # Load Dataset\n",
    "        print(\"Loading dataset\")\n",
    "        with open(args.train_data, 'r') as f:\n",
    "            train_words = [line.strip() for line in f]\n",
    "        with open(args.evaluation_data, 'r') as f:\n",
    "            test_words = [line.strip() for line in f]\n",
    "        with open(args.train_tags, 'r') as f:\n",
    "            train_tags = [line.strip() for line in f]\n",
    "        with open(args.evaluation_tags, 'r') as f:\n",
    "            test_tags = [line.strip() for line in f]\n",
    "\n",
    "        # Normalize text\n",
    "        print(\"Normalizing text and producing vocabularies.\")\n",
    "        train_words = [normalize(line) for line in train_words]\n",
    "        test_words = [normalize(line) for line in test_words]\n",
    "\n",
    "        # Produce vocabularies\n",
    "        word_vocab, idx2word, word2idx = produce_vocab(train_words, min_freq=args.min_freq)\n",
    "        tags_vocab, idx2tag, tag2idx  = produce_vocab(train_tags, min_freq=args.min_freq)\n",
    "        print(\"Training word vocabulary has {:,} unique tokens.\".format(len(word_vocab)))\n",
    "        print(\"Training tags vocabulary has {:,} unique tokens.\".format(len(tags_vocab)))\n",
    "\n",
    "        # Produce sets\n",
    "        X_train = proc_set(train_words, word2idx, word_vocab, msl=args.msl)\n",
    "        y_train = proc_set(train_tags , tag2idx,  tags_vocab,  msl=args.msl)\n",
    "        X_test = proc_set(test_words, word2idx, word_vocab, msl=args.msl)\n",
    "        y_test = proc_set(test_tags , tag2idx,  tags_vocab,  msl=args.msl)\n",
    "\n",
    "        # Convert to tensors\n",
    "        X_train, y_train = torch.LongTensor(X_train), torch.LongTensor(y_train)\n",
    "        X_test, y_test = torch.LongTensor(X_test), torch.LongTensor(y_test)\n",
    "\n",
    "        # Produce dataloaders\n",
    "        train_set = datautils.TensorDataset(X_train, y_train)\n",
    "        test_set = datautils.TensorDataset(X_test, y_test)\n",
    "        train_sampler = datautils.RandomSampler(train_set)\n",
    "        train_loader = datautils.DataLoader(train_set, sampler=train_sampler, batch_size=args.bs)\n",
    "        test_loader = datautils.DataLoader(test_set, shuffle=False, batch_size=args.bs)\n",
    "\n",
    "        print(\"Training batches: {}\\nEvaluation batches: {}\".format(len(train_loader), len(test_loader)))\n",
    "\n",
    "        # Training setup\n",
    "        model = LSTMTagger(word_vocab_sz=len(word_vocab), \n",
    "                           tag_vocab_sz=len(tags_vocab), \n",
    "                           embedding_dim=args.embedding_dim, \n",
    "                           hidden_dim=args.hidden_dim, \n",
    "                           dropout=args.dropout,\n",
    "                           num_layers=args.num_layers,\n",
    "                           recur_dropout=args.recur_dropout,\n",
    "                           bidirectional=args.bidirectional).to(device)\n",
    "        model.apply(init_weights)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=tag2idx['<pad>'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "        print(\"Model has {:,} trainable parameters.\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))   \n",
    "\n",
    "        # Training\n",
    "        for e in range(1, args.epochs + 1):\n",
    "            model.train()\n",
    "            train_loss, train_acc = 0, 0\n",
    "            for x, y in tqdm(train_loader):\n",
    "                x, y = x.transpose(1, 0).to(device), y.transpose(1, 0).to(device)\n",
    "                out = model(x)\n",
    "                loss = criterion(out.flatten(0, 1), y.flatten(0))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                train_acc += accuracy(out, y, tag2idx)\n",
    "            train_loss /= len(train_loader)\n",
    "            train_acc /= len(train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            test_loss, test_acc = 0, 0\n",
    "            for x, y in tqdm(test_loader):\n",
    "                with torch.no_grad():\n",
    "                    x, y = x.transpose(1, 0).to(device), y.transpose(1, 0).to(device)\n",
    "                    out = model(x)\n",
    "                    loss = criterion(out.flatten(0, 1), y.flatten(0))\n",
    "                test_loss += loss.item()\n",
    "                test_acc += accuracy(out, y, tag2idx)\n",
    "            test_loss /= len(test_loader)\n",
    "            test_acc /= len(test_loader)\n",
    "\n",
    "            print(\"Epoch {:4} | Train Loss {:.4f} | Train Acc {:.2f}% | Test Loss {:.4f} | Test Acc {:.2f}%\".format(e, train_loss, train_acc, test_loss, test_acc))  \n",
    "        \n",
    "        # Save model\n",
    "        if args.overwrite_save_directory:\n",
    "            if os.path.exists(args.checkpoint): os.system('rm -r '+ args.checkpoint + '/')\n",
    "\n",
    "        print('Saving model and vocabularies.')\n",
    "        os.mkdir(args.checkpoint)\n",
    "        with open(args.checkpoint + '/model.bin', 'wb') as f:\n",
    "            torch.save(model.state_dict(), f)\n",
    "        with open(args.checkpoint + '/settings.bin', 'wb') as f:\n",
    "            torch.save([word_vocab, word2idx, idx2word, tags_vocab, tag2idx, idx2tag, args.msl, \n",
    "                        args.embedding_dim, args.hidden_dim, args.dropout, args.bidirectional, \n",
    "                        args.num_layers, args.recur_dropout], f)\n",
    "    '''\n",
    "    #if args.do_predict:\n",
    "        # Load the vocabularies\n",
    "    with open('checkpoint/settings.bin', 'rb') as f:\n",
    "        word_vocab, word2idx, idx2word, tags_vocab, tag2idx, idx2tag, msl, embedding_dim, hidden_dim, dropout, bidirectional, num_layers, recur_dropout = torch.load(f)\n",
    "\n",
    "        # Produce a blank model\n",
    "    model = LSTMTagger(word_vocab_sz=len(word_vocab), \n",
    "                        tag_vocab_sz=len(tags_vocab), \n",
    "                        embedding_dim=embedding_dim, \n",
    "                        hidden_dim=hidden_dim, \n",
    "                        dropout=dropout,\n",
    "                        num_layers=num_layers,\n",
    "                        recur_dropout=recur_dropout,\n",
    "                        bidirectional=bidirectional)\n",
    "\n",
    "    # Load checkpoints and put the model in eval mode\n",
    "    with open('checkpoint/model.bin', 'rb') as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "    model = model.cpu()\n",
    "    model.eval();\n",
    "\n",
    "    preds = predict(args.sentence, word2idx, idx2tag, word_vocab, msl, model)\n",
    "    print(preds)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
