{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8b4b7d",
   "metadata": {},
   "source": [
    "# Testing Different Monolingual Filipino and English Part of Speech (POS) Taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62008160",
   "metadata": {},
   "source": [
    "### PLEASE TAKE NOTE!!!\n",
    "- [IMPORTANT] Always refresh kernel, clear outputs, and save before exiting to avoid git conflicts\n",
    "- Current formatting of this .ipynb is not final, will reformat when testing sample data from FilWordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a8e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lingua import Language, LanguageDetectorBuilder #used for language identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfbc41b",
   "metadata": {},
   "source": [
    "Initialize language Identification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [Language.ENGLISH, Language.TAGALOG]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7935269c",
   "metadata": {},
   "source": [
    "Initialize the dataframe that will hold the sentences and its pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e071b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_format = {\n",
    "    \"text\": [],\n",
    "    \"general_tags\": [],\n",
    "    \"specific_tags\": [],\n",
    "    \"token_tagset\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_texts_combi1_ff = pd.DataFrame(df_format)\n",
    "tagged_texts_combi2_ff = pd.DataFrame(df_format)\n",
    "\n",
    "tagged_texts_combi1_sf = pd.DataFrame(df_format)\n",
    "tagged_texts_combi2_sf = pd.DataFrame(df_format)\n",
    "\n",
    "display(tagged_texts_combi1_ff)\n",
    "display(tagged_texts_combi2_ff)\n",
    "\n",
    "display(tagged_texts_combi1_sf)\n",
    "display(tagged_texts_combi2_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c92704",
   "metadata": {},
   "source": [
    "## Loading the test data\n",
    "\n",
    "Let us load the .json input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataframe = pd.read_json(\"input_data.json\")\n",
    "display(input_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e57ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_temp = []\n",
    "tags_temp = []\n",
    "input_sentence = []\n",
    "\n",
    "for i in range(len(input_dataframe)):\n",
    "    tokens_temp.clear()\n",
    "    tags_temp.clear()\n",
    "    \n",
    "    for j in range(input_dataframe.iloc[i].count()):\n",
    "        tokens_temp.append(input_dataframe.iloc[i][j].__getitem__(\"token\"))\n",
    "        tags_temp.append(input_dataframe.iloc[i][j].__getitem__(\"tag\"))\n",
    "        \n",
    "    sentence_temp = ' '.join([str(item) for item in tokens_temp])\n",
    "    \n",
    "    input_sentence.append(sentence_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c70e96d",
   "metadata": {},
   "source": [
    "## POS TAGGERS\n",
    "\n",
    "Let us import the monolingual taggers. Flair for english pos tagger and FSPOST for filipino pos tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd55134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAIR POS TAGGER\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "flair_tagger = SequenceTagger.load(\"flair/pos-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3df152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY POS TAGGER\n",
    "import spacy\n",
    "\n",
    "spacy_tagger = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d623e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSPOST POS TAGGER\n",
    "import os\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "# These are Windows formatted directories\n",
    "#model = 'model//filipino-left5words-owlqn2-distsim-pref6-inf2.tagger'\n",
    "#jar = 'lib//stanford-postagger.jar'\n",
    "\n",
    "# These are Linux formatted directories\n",
    "model = 'model/filipino-left5words-owlqn2-distsim-pref6-inf2.tagger'\n",
    "jar = 'lib/stanford-postagger.jar'\n",
    "\n",
    "fspost = StanfordPOSTagger(model, path_to_jar=jar)  # Load Tagger Model\n",
    "fspost._SEPARATOR = '|'  # Set separator for proper tuple formatting (word, tag)\n",
    "\n",
    "def set_java_path(file_path):\n",
    "    \"\"\"\n",
    "    Function for setting java path to make Stanford POS Tagger work. Makes use of the 'os' library. Input \"\" to use\n",
    "    default java path, otherwise set the location.\n",
    "    Args:\n",
    "        file_path (str): The java file path / location.\n",
    "    \"\"\"\n",
    "    if file_path == \"\":\n",
    "        java_path = \"C:/Program Files/Java/jdk1.8.0_111/bin/java.exe\"\n",
    "        print(\"Java path set by default\")\n",
    "    else:\n",
    "        java_path = file_path\n",
    "        print(\"Java path set from given\")\n",
    "    os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "def tag_string(sentence):\n",
    "    \"\"\"\n",
    "    Function for tagging a sentence/string. Output is a (word, pos) tuple. To output a POS-only string, enclose this\n",
    "    function with 'format_pos' function. Ex. fspost.format_pos(fspost.tag_string('this is a string')). Same goes for\n",
    "    Stanford's word|tag notation, use 'format_stanford' function.\n",
    "    Args:\n",
    "        sentence (str): The string to be tagged.\n",
    "    Returns:\n",
    "        tagged_string: a list of string tokens containing POS labeled (word, pos) tuples.\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()  # Tokenize Sentence by whitespaces\n",
    "    # print(tokens)\n",
    "    tagged_string = fspost.tag(tokens)\n",
    "    return tagged_string\n",
    "\n",
    "def tag_string_list(sentence_list):\n",
    "    \"\"\"\n",
    "    Function for tagging a list of sentences. Output is a list of (word, pos) tuple. To output a POS-only string,\n",
    "    enclose the elements in this function with 'format_pos' function. Same goes for Stanford's word|tag notation, use\n",
    "    'format_stanford' function.\n",
    "    Args:\n",
    "        sentence_list (list): The list of strings to be tagged.\n",
    "    Returns:\n",
    "        tagged_list: a list of strings containing POS labelled (word, pos) tuples.\n",
    "    \"\"\"\n",
    "    progress_ctr = 0\n",
    "    tagged_list = []  # Initialize an empty list\n",
    "    for sentence in sentence_list:\n",
    "        tagged_tuple = tag_string(sentence)  # Tag each sentence in the list\n",
    "        tagged_list.append(tagged_tuple)  # Insert tagged sentence in the new list\n",
    "        progress_ctr += 1\n",
    "        print(progress_ctr, \"/\", len(sentence_list))  # Progress Counter\n",
    "    return tagged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba20a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOWS\n",
    "# set_java_path(\"C:/Program Files/Java/jdk-19/bin/java.exe\")\n",
    "\n",
    "# LINUX\n",
    "set_java_path(\"/usr/lib/jvm/java-11-openjdk-amd64/bin/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355279da",
   "metadata": {},
   "source": [
    "### Create functions to be used for POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea912637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the eng POS tag (Flair version)\n",
    "def eng_tagger_flair(input_string):\n",
    "    sentence = Sentence(input_string)\n",
    "    flair_tagger.predict(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a14812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_tagger_spacy(input_string):\n",
    "    return spacy_tagger(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the fil POS tag\n",
    "def fil_tagger(input_string):\n",
    "    return tag_string(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the specific tag into generalized tag\n",
    "def convert_eng(pos_tag):\n",
    "    if(pos_tag == \"NN\" or pos_tag == \"NNS\"):\n",
    "        return \"NOUN\"\n",
    "    elif(pos_tag == \"NNP\" or pos_tag == \"NNPS\"):\n",
    "        return \"PROPN\"\n",
    "    elif(pos_tag == \"PRP\" or pos_tag == \"PRP$\" or pos_tag == \"WP\" or pos_tag == \"WP$\"):\n",
    "        return \"PR\"\n",
    "    elif(pos_tag == \"DT\" or pos_tag == \"WDT\"):\n",
    "        return \"DT\"\n",
    "    elif(pos_tag == \"CC\"):\n",
    "        return \"CONJ\"\n",
    "    elif(pos_tag == \"IN\"):\n",
    "        return \"IN\"\n",
    "    elif(pos_tag == \"VB\" or pos_tag == \"VBD\" or pos_tag == \"VBG\" or pos_tag == \"VBN\" \n",
    "         or pos_tag == \"VBP\" or pos_tag == \"VBZ\"):\n",
    "        return \"VB\"\n",
    "    elif(pos_tag == \"JJ\" or pos_tag == \"JJR\" or pos_tag == \"JJS\"):\n",
    "        return \"JJ\"\n",
    "    elif(pos_tag == \"CD\"):\n",
    "        return \"CD\"\n",
    "    elif(pos_tag == \"RB\" or pos_tag == \"RBR\" or pos_tag == \"RBS\" or pos_tag == \"WRB\" or pos_tag == \"RP\"):\n",
    "        return \"RB\"\n",
    "    elif(pos_tag == \"UH\"):\n",
    "        return \"UH\"\n",
    "    elif(pos_tag == \"FW\"):\n",
    "        return \"FW\"\n",
    "    elif(pos_tag == \".\" or pos_tag == \",\" or pos_tag == \":\" or pos_tag == \"NFP\" or pos_tag == \"(\" or pos_tag == \")\"\n",
    "        or pos_tag == \"''\" or pos_tag == '\"\"' or pos_tag == \"``\" or pos_tag == \"`\" or pos_tag == \"-RRB-\"\n",
    "        or pos_tag == \"-LRB-\"):\n",
    "        return \"PUNC\"\n",
    "    elif(pos_tag == \"HYPH\" or pos_tag == \"SYM\" or pos_tag == \"$\" or pos_tag == \"\\\"\" or pos_tag == \"LS\"):\n",
    "        return \"SYM\"\n",
    "    else:\n",
    "        return pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04781d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the specific tag into generalized tag\n",
    "def convert_fil(pos_tag):\n",
    "    if(pos_tag == \"NNC\" or pos_tag == \"NNCA\"):\n",
    "        return \"NOUN\"\n",
    "    elif(pos_tag == \"NNP\" or pos_tag == \"NNPA\"):\n",
    "        return \"PROPN\"\n",
    "    elif(pos_tag == \"PRS\" or pos_tag == \"PRP\" or pos_tag == \"PRSP\" or pos_tag == \"PRO\"\n",
    "        or pos_tag == \"PRQ\" or pos_tag == \"PRQP\" or pos_tag == \"PRL\" or pos_tag == \"PRC\"\n",
    "        or pos_tag == \"PRF\" or pos_tag == \"PRI\"):\n",
    "        return \"PR\"\n",
    "    elif(pos_tag == \"DTC\" or pos_tag == \"DTCP\" or pos_tag == \"DTP\" or pos_tag == \"DTPP\"):\n",
    "        return \"DT\"\n",
    "    elif(pos_tag == \"LM\"):\n",
    "        return \"LM\"\n",
    "    elif(pos_tag == \"CCT\" or pos_tag == \"CCR\" or pos_tag == \"CCB\" or pos_tag == \"CCA\"):\n",
    "        return \"CONJ\"\n",
    "    elif(pos_tag == \"CCP\"):\n",
    "        return \"CCP\"\n",
    "    elif(pos_tag == \"CCU\"):\n",
    "        return \"IN\"\n",
    "    elif(pos_tag == \"VBW\" or pos_tag == \"VBS\" or pos_tag == \"VBH\" or pos_tag == \"VBN\"\n",
    "        or pos_tag == \"VBTS\" or pos_tag == \"VBTR\" or pos_tag == \"VBTF\" or pos_tag == \"VBTP\"\n",
    "        or pos_tag == \"VBAF\" or pos_tag == \"VBOF\" or pos_tag == \"VBOB\" or pos_tag == \"VBOL\"\n",
    "        or pos_tag == \"VBOI\" or pos_tag == \"VBRF\"):\n",
    "        return \"VB\"\n",
    "    elif(pos_tag == \"JJD\" or pos_tag == \"JJC\" or pos_tag == \"JJCC\" or pos_tag == \"JJCS\" or pos_tag == \"JJCN\"):\n",
    "        return \"JJ\"\n",
    "    elif(pos_tag == \"JJN\" or pos_tag == \"CDB\"):\n",
    "        return \"CD\"\n",
    "    elif(pos_tag == \"RBD\" or pos_tag == \"RBN\" or pos_tag == \"RBK\" or pos_tag == \"RBP\"\n",
    "        or pos_tag == \"RBB\" or pos_tag == \"RBR\" or pos_tag == \"RBQ\" or pos_tag == \"RBT\"\n",
    "        or pos_tag == \"RBF\" or pos_tag == \"RBW\" or pos_tag == \"RBM\" or pos_tag == \"RBL\"\n",
    "        or pos_tag == \"RBI\" or pos_tag == \"RBS\"):\n",
    "        return \"RB\"\n",
    "    elif(pos_tag == \"RBJ\"):\n",
    "        return \"UH\"\n",
    "    elif(pos_tag == \"FW\"):\n",
    "        return \"FW\"\n",
    "    elif(pos_tag == \"PMP\" or pos_tag == \"PME\" or pos_tag == \"PMQ\" or pos_tag == \"PMC\" or pos_tag == \"PMSC\"):\n",
    "        return \"PUNC\"\n",
    "    elif(pos_tag == \"PMS\"):\n",
    "        return \"SYM\"\n",
    "    else:\n",
    "        return pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "langid = [] # will be used to store language id\n",
    "\n",
    "def reset_variables(general, specific, token_tagset):\n",
    "    #langid.clear()\n",
    "    general.clear()\n",
    "    specific.clear()\n",
    "    token_tagset.clear()\n",
    "    return\n",
    "\n",
    "def reset_variables_combi2(gen_eng, spec_eng, gen_fil, spec_fil):\n",
    "    gen_eng.clear()\n",
    "    spec_eng.clear()\n",
    "    gen_fil.clear()\n",
    "    spec_fil.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456baed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to be used for Flair-FSPOST Tagger\n",
    "\n",
    "pos_tags_general_ff = [] # will be used to store generalized pos tags\n",
    "pos_tags_specific_ff = [] # will be used to store specific pos tags\n",
    "token_tagset_ff = [] # will be used to store the name of the tagset used for specific tags\n",
    "\n",
    "# Temporary lists to be used for combi 2\n",
    "pos_tags_general_eng_ff = []\n",
    "pos_tags_specific_eng_ff = []\n",
    "pos_tags_general_fil_ff = []\n",
    "pos_tags_specific_fil_ff = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854669e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to be used for Spacy-FSPOST Tagger\n",
    "\n",
    "pos_tags_general_sf = [] # will be used to store generalized pos tags\n",
    "pos_tags_specific_sf = [] # will be used to store specific pos tags\n",
    "token_tagset_sf = [] # will be used to store the name of the tagset used for specific tags\n",
    "\n",
    "# Temporary lists to be used for combi 2\n",
    "pos_tags_general_eng_sf = []\n",
    "pos_tags_specific_eng_sf = []\n",
    "pos_tags_general_fil_sf = []\n",
    "pos_tags_specific_fil_sf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_dataframe(input_sentence, general_tags, specific_tags, tagset, tagged_texts):\n",
    "    #tagged_texts = tagged_texts.append({\"text\": input_sentence, \"general_tags\": np.array(general_tags),\n",
    "                                       # \"specific_tags\": np.array(specific_tags), \"token_tagset\": np.array(tagset)},\n",
    "                                       #ignore_index = True)\n",
    "    tagged_texts = pd.concat([tagged_texts, pd.DataFrame.from_records([{\"text\": input_sentence,\n",
    "                            \"general_tags\": np.array(general_tags), \"specific_tags\": np.array(specific_tags),\n",
    "                            \"token_tagset\": np.array(tagset)}])], ignore_index = True)\n",
    "    return tagged_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c8077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_string(string_list):\n",
    "    return ' '.join([str(item) for item in string_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c41330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_text_with_punc(input_text):\n",
    "    input_text_tokenized = nltk.word_tokenize(input_text)\n",
    "    return input_text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421acd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_per_token(text_wo_punc):\n",
    "    langid = []\n",
    "    #Identifies the language of each tokens to determine which tagger to use\n",
    "    for i in range(len(text_wo_punc)):\n",
    "        langid.append(detector.detect_language_of(text_wo_punc[i]))\n",
    "        \n",
    "    return langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_csv(dataframes, output_name):\n",
    "    dataframes['general_tags'] = dataframes['general_tags'].map(list)\n",
    "    dataframes['specific_tags'] = dataframes['specific_tags'].map(list)\n",
    "    dataframes['token_tagset'] = dataframes['token_tagset'].map(list)\n",
    "    dataframes.to_csv(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e28df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isMultipleTags(tag):\n",
    "    if(tag.__contains__('_')):\n",
    "        new_tag = tag.split('_')\n",
    "        return new_tag[0]\n",
    "    else:\n",
    "        return tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc923d",
   "metadata": {},
   "source": [
    "## Language Identification then Monolingual Tagging (COMBI 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f8934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_id_then_mono_tag(input_string):\n",
    "    \n",
    "    input_text_tokenized = tokenized_text_with_punc(input_string)\n",
    "    \n",
    "    # reset temporary variables\n",
    "    reset_variables(pos_tags_general_ff, pos_tags_specific_ff, token_tagset_ff) # Flair-FSPOST version\n",
    "    reset_variables(pos_tags_general_sf, pos_tags_specific_sf, token_tagset_sf) # Spacy-FSPOST version\n",
    "    \n",
    "    langid = get_lang_per_token(input_text_tokenized)\n",
    "    \n",
    "    for i in range(len(input_text_tokenized)):\n",
    "        if(langid[i] == Language.TAGALOG):\n",
    "            token = fil_tagger(input_text_tokenized[i])\n",
    "            new_token = isMultipleTags(token[0][1])\n",
    "            \n",
    "            # Flair-FSPOST dataframes\n",
    "            pos_tags_general_ff.append(convert_fil(new_token))\n",
    "            pos_tags_specific_ff.append(new_token)\n",
    "            token_tagset_ff.append(\"MGNN\")\n",
    "            \n",
    "            # Spacy-FSPOST dataframes\n",
    "            pos_tags_general_sf.append(convert_fil(new_token))\n",
    "            pos_tags_specific_sf.append(new_token)\n",
    "            token_tagset_sf.append(\"MGNN\")\n",
    "\n",
    "        elif(langid[i] == Language.ENGLISH):\n",
    "            \n",
    "            # Flair-FSPOST dataframes\n",
    "            token = eng_tagger_flair(input_text_tokenized[i])\n",
    "            new_token = isMultipleTags(token.get_labels('pos')[0].value)\n",
    "            pos_tags_general_ff.append(convert_eng(new_token))\n",
    "            pos_tags_specific_ff.append(new_token)\n",
    "            token_tagset_ff.append(\"Flair\")\n",
    "            \n",
    "             # Spacy-FSPOST dataframes\n",
    "            token = eng_tagger_spacy(input_text_tokenized[i])\n",
    "            new_token = isMultipleTags(token[0].tag_)\n",
    "            pos_tags_general_sf.append(convert_eng(new_token))\n",
    "            pos_tags_specific_sf.append(new_token)\n",
    "            token_tagset_sf.append(\"Spacy\")\n",
    "\n",
    "        else:\n",
    "            token = fil_tagger(input_text_tokenized[i])\n",
    "            new_token = isMultipleTags(token[0][1])\n",
    "            \n",
    "            # Flair-FSPOST dataframes\n",
    "            pos_tags_general_ff.append(convert_fil(new_token))\n",
    "            pos_tags_specific_ff.append(new_token)\n",
    "            token_tagset_ff.append(\"MGNN\")\n",
    "            \n",
    "            # Spacy-FSPOST dataframes\n",
    "            pos_tags_general_sf.append(convert_fil(new_token))\n",
    "            pos_tags_specific_sf.append(new_token)\n",
    "            token_tagset_sf.append(\"MGNN\")\n",
    "        \n",
    "    global tagged_texts_combi1_ff\n",
    "    temp = tagged_texts_combi1_ff\n",
    "    tagged_texts_combi1_ff = append_to_dataframe(input_string, pos_tags_general_ff,\n",
    "                                              pos_tags_specific_ff, token_tagset_ff, temp)\n",
    "    \n",
    "    global tagged_texts_combi1_sf\n",
    "    temp = tagged_texts_combi1_sf\n",
    "    tagged_texts_combi1_sf = append_to_dataframe(input_string, pos_tags_general_sf,\n",
    "                                                 pos_tags_specific_sf, token_tagset_sf, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c795ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in range(len(input_sentence)):\n",
    "    lang_id_then_mono_tag(input_sentence[i])\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"pass \", j)\n",
    "        j = j + 1\n",
    "display(tagged_texts_combi1_ff)\n",
    "display(tagged_texts_combi1_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68321ce3",
   "metadata": {},
   "source": [
    "## Monolingual Tagging then Language Identification (COMBI 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fcdfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mono_tag_then_lang_id(input_string):\n",
    "    # Resets temp variables FLair version\n",
    "    reset_variables(pos_tags_general_ff, pos_tags_specific_ff, token_tagset_ff)\n",
    "    reset_variables_combi2(pos_tags_general_eng_ff, pos_tags_specific_eng_ff,\n",
    "                           pos_tags_general_fil_ff, pos_tags_specific_fil_ff)\n",
    "    \n",
    "    # Resets temp variables Spacy version\n",
    "    reset_variables(pos_tags_general_sf, pos_tags_specific_sf, token_tagset_sf)\n",
    "    reset_variables_combi2(pos_tags_general_eng_sf, pos_tags_specific_eng_sf,\n",
    "                           pos_tags_general_fil_sf, pos_tags_specific_fil_sf)\n",
    "    \n",
    "    # Tokenized sentences\n",
    "    input_text_tokenized = tokenized_text_with_punc(input_string)\n",
    "    \n",
    "    # Flair (English) pos tagging\n",
    "    token_eng_flair = eng_tagger_flair(input_string)\n",
    "    # Spacy (English) pos tagging\n",
    "    token_eng_spacy = eng_tagger_spacy(input_string)\n",
    "    \n",
    "    # Store tags to temporary variables (ENGLISH - Flair Tagger)\n",
    "    for i in range(len(token_eng_flair.get_labels('pos'))):\n",
    "        new_token = isMultipleTags(token_eng_flair.get_labels('pos')[i].value)\n",
    "        pos_tags_general_eng_ff.append(convert_eng(new_token))\n",
    "        pos_tags_specific_eng_ff.append(new_token)\n",
    "        \n",
    "   # Store tags to temporary variables (ENGLISH - Spacy Tagger)\n",
    "    for i in range(len(token_eng_spacy)):\n",
    "        new_token = isMultipleTags(token_eng_spacy[i].tag_)\n",
    "        pos_tags_general_eng_sf.append(convert_eng(new_token))\n",
    "        pos_tags_specific_eng_sf.append(new_token)\n",
    "         \n",
    "        \n",
    "    # FSPOST (Filipino) pos tagging\n",
    "    token_fil = fil_tagger(input_string)\n",
    "    \n",
    "    # Store tags to temporary variables (FILIPINO Tagger)\n",
    "    for i in range(len(token_fil)):\n",
    "        new_token = isMultipleTags(token_fil[i][1])\n",
    "        \n",
    "        # Store filipino tags on Flair-FSPOST dataframe\n",
    "        pos_tags_general_fil_ff.append(convert_fil(new_token))\n",
    "        pos_tags_specific_fil_ff.append(new_token)\n",
    "        \n",
    "        # Store filipino tags on Spacy-FSPOST dataframe\n",
    "        pos_tags_general_fil_sf.append(convert_fil(new_token))\n",
    "        pos_tags_specific_fil_sf.append(new_token)\n",
    "    \n",
    "    # Get Languages per token ( Language Identification )\n",
    "    langid = get_lang_per_token(input_text_tokenized)\n",
    "    \n",
    "    for i in range(len(input_text_tokenized)):\n",
    "        if(langid[i] == Language.TAGALOG):\n",
    "            pos_tags_general_ff.append(pos_tags_general_fil_ff[i])\n",
    "            pos_tags_specific_ff.append(pos_tags_specific_fil_ff[i])\n",
    "            token_tagset_ff.append(\"MGNN\")\n",
    "            \n",
    "            pos_tags_general_sf.append(pos_tags_general_fil_sf[i])\n",
    "            pos_tags_specific_sf.append(pos_tags_specific_fil_sf[i])\n",
    "            token_tagset_sf.append(\"MGNN\")\n",
    "\n",
    "        elif(langid[i] == Language.ENGLISH):\n",
    "            pos_tags_general_ff.append(pos_tags_general_eng_ff[i])\n",
    "            pos_tags_specific_ff.append(pos_tags_specific_eng_ff[i])\n",
    "            token_tagset_ff.append(\"Flair\")\n",
    "            \n",
    "            pos_tags_general_sf.append(pos_tags_general_eng_sf[i])\n",
    "            pos_tags_specific_sf.append(pos_tags_specific_eng_sf[i])\n",
    "            token_tagset_sf.append(\"Spacy\")\n",
    "\n",
    "        else:\n",
    "            pos_tags_general_ff.append(pos_tags_general_fil_ff[i])\n",
    "            pos_tags_specific_ff.append(pos_tags_specific_fil_ff[i])\n",
    "            token_tagset_ff.append(\"MGNN\")\n",
    "            \n",
    "            pos_tags_general_sf.append(pos_tags_general_fil_sf[i])\n",
    "            pos_tags_specific_sf.append(pos_tags_specific_fil_sf[i])\n",
    "            token_tagset_sf.append(\"MGNN\")\n",
    "        \n",
    "        \n",
    "    global tagged_texts_combi2_ff\n",
    "    temp = tagged_texts_combi2_ff\n",
    "    tagged_texts_combi2_ff = append_to_dataframe(input_string, pos_tags_general_ff,\n",
    "                                              pos_tags_specific_ff, token_tagset_ff, temp)\n",
    "    \n",
    "    global tagged_texts_combi2_sf\n",
    "    temp = tagged_texts_combi2_sf\n",
    "    tagged_texts_combi2_sf = append_to_dataframe(input_string, pos_tags_general_sf,\n",
    "                                              pos_tags_specific_sf, token_tagset_sf, temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e39498",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in range(len(input_sentence)):\n",
    "    #try:\n",
    "    mono_tag_then_lang_id(input_sentence[i])\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"pass \", j)\n",
    "        j = j + 1\n",
    "            \n",
    "    #except:\n",
    "    #print(i, ': ', input_sentence[i])\n",
    "        \n",
    "display(tagged_texts_combi2_ff)\n",
    "display(tagged_texts_combi2_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c3e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_to_csv(tagged_texts_combi1_ff, \"Flair-FSPOST-Combination-1.csv\")\n",
    "dataframe_to_csv(tagged_texts_combi2_ff, \"Flair-FSPOST-Combination-2.csv\")\n",
    "\n",
    "dataframe_to_csv(tagged_texts_combi1_sf, \"Spacy-FSPOST-Combination-1.csv\")\n",
    "dataframe_to_csv(tagged_texts_combi2_sf, \"Spacy-FSPOST-Combination-2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ad85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tagged_texts_combi1_ff)\n",
    "display(tagged_texts_combi2_ff)\n",
    "display(tagged_texts_combi1_sf)\n",
    "display(tagged_texts_combi2_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb7104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = tag_string(\"Noong nakaraang Nobyembre , isang tuta na kakaiba rin ang hitsura ang isinilang naman sa Mati city .\")\n",
    "#test = tag_string(\"Pumanaw ang 65 taong gulang\")\n",
    "test = tag_string(input_sentence[1])\n",
    "print(test[2].get_item)\n",
    "test[0].__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9fb4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_tagset)\n",
    "display(input_sentence[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff40814",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tagged_texts_combi1['general_tags'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a7eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a99184b",
   "metadata": {},
   "source": [
    "# TO BE REMOVED EVERYTHING BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25537417",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_variables(sample_text_pos_tags_general, sample_text_pos_tags_specific)\n",
    "reset_variables_combi2(sample_text_pos_tags_general_eng, sample_text_pos_tags_specific_eng,\n",
    "                       sample_text_pos_tags_general_fil, sample_text_pos_tags_specific_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flair (English) pos tagging\n",
    "token_eng = eng_tagger(sample_text)\n",
    "\n",
    "for i in range(len(token_eng.get_labels('pos'))):\n",
    "    sample_text_pos_tags_general_eng.append(convert_eng(token_eng.get_labels('pos')[i].value))\n",
    "    sample_text_pos_tags_specific_eng.append(convert_eng(token_eng.get_labels('pos')[i].value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSPOST (Filipino) pos tagging\n",
    "sentence = nltk.word_tokenize(sample_text)\n",
    "sentence = join_string(sentence)\n",
    "token_fil = fil_tagger(sentence)\n",
    "\n",
    "for i in range(len(token_fil)):\n",
    "    sample_text_pos_tags_general_fil.append(convert_fil(token_fil[i][1]))\n",
    "    sample_text_pos_tags_specific_fil.append(convert_fil(token_fil[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Language identification model cannot identify punctuations or symbol.\n",
    "#Create a copy of the sentence without punctuations or symbols\n",
    "text_without_punc = sample_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#Tokenize the sample text without punctuation\n",
    "text_without_punc_tokenized = nltk.word_tokenize(text_without_punc)\n",
    "sample_text_tokenized = nltk.word_tokenize(sample_text)\n",
    "\n",
    "#IF ABOVE CODE HAS LOOKUP ERROR: RUN CODE ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243032f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifies the language of each tokens to determine which tagger to use\n",
    "for i in range(len(text_without_punc_tokenized)):\n",
    "    sample_text_langid.append(detector.detect_language_of(text_without_punc_tokenized[i]))\n",
    "    print(text_without_punc_tokenized[i], \": \", sample_text_langid[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9823a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in range(len(sample_text_tokenized)):\n",
    "    if(sample_text_tokenized[i] == text_without_punc_tokenized[j]):\n",
    "        if(sample_text_langid[j] == Language.TAGALOG):\n",
    "            sample_text_pos_tags_general.append(sample_text_pos_tags_general_fil[i])\n",
    "            sample_text_pos_tags_specific.append(sample_text_pos_tags_specific_fil[i])\n",
    "            \n",
    "        elif(sample_text_langid[j] == Language.ENGLISH):\n",
    "            token = eng_tagger(text_without_punc_tokenized[j])\n",
    "            sample_text_pos_tags_general.append(convert_eng(token.get_labels('pos')[0].value))\n",
    "            sample_text_pos_tags_specific.append(token.get_labels('pos')[0].value)\n",
    "            \n",
    "        j = j + 1\n",
    "        \n",
    "        if(j == len(text_without_punc_tokenized)):\n",
    "            j = 0\n",
    "    else:\n",
    "        token = fil_tagger(sample_text_tokenized[i])\n",
    "        sample_text_pos_tags_general.append(sample_text_pos_tags_general_fil[i])\n",
    "        sample_text_pos_tags_specific.append(sample_text_pos_tags_specific_fil[i])\n",
    "        \n",
    "    print(sample_text_tokenized[i], \": General POS Tag -> \", sample_text_pos_tags_general[i], \" - Specific POS Tag -> \", sample_text_pos_tags_specific[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_text_pos_tags_specific)\n",
    "append_to_dataframe(sample_text, sample_text_pos_tags_general, sample_text_pos_tags_specific, tagged_texts_combi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e269e0f",
   "metadata": {},
   "source": [
    "## DO NOT RUN ANYTHING BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = Sentence(sample_text)\n",
    "#tagger.predict(test)\n",
    "\n",
    "test = eng_tagger(\"ako\")\n",
    "\n",
    "#print(test.get_labels('pos')[0])\n",
    "\n",
    "#print(test[0].get_labels('pos').value)\n",
    "\n",
    "label = test.get_labels('pos')[0].value\n",
    "print(test.get_labels('pos')[0].value)\n",
    "\n",
    "#for label in test.get_labels('pos'):\n",
    "    #print(label.value)\n",
    "    \n",
    "#print(test)\n",
    "#print(test.to_tagged_string())\n",
    "#for entity in test.get_spans('pos'):\n",
    "    #print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e7742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = tag_string(sample_text)\n",
    "test = fil_tagger(sample_text)\n",
    "print(test[0])\n",
    "#print(dir(test2[0].__getattribute__('pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b296133",
   "metadata": {},
   "source": [
    "### Flair Testing (with FW tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22542e6",
   "metadata": {},
   "source": [
    "Import Flair and tagger to use (pos-english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "# gian was here\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/pos-english\")\n",
    "#tagger = SequenceTagger.load(\"pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24365057",
   "metadata": {},
   "source": [
    "Generate POS tags with infinite loop for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cddcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type C or c to cancel loop\n",
    "while True:\n",
    "    input_sentence = input(\"Enter sample sentence: \")\n",
    "    \n",
    "    if input_sentence == \"c\" or input_sentence == \"C\":\n",
    "        break\n",
    "        \n",
    "    sentence_test = Sentence(input_sentence)\n",
    "    tagger.predict(sentence_test)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(sentence_test)\n",
    "    \n",
    "    \n",
    "    # print predicted NER spans\n",
    "    print('The following NER tags are found:')\n",
    "    # iterate over entities and print\n",
    "    for entity in sentence_test.get_spans('pos'):\n",
    "        print(entity)\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73360371",
   "metadata": {},
   "source": [
    "### FSPOST (Go & Nocon, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af9dcf",
   "metadata": {},
   "source": [
    "Use FSPOST pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fee2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "# These are Windows formatted directories\n",
    "#model = 'model//filipino-left5words-owlqn2-distsim-pref6-inf2.tagger'\n",
    "#jar = 'lib//stanford-postagger.jar'\n",
    "\n",
    "# These are Linux formatted directories\n",
    "model = 'model/filipino-left5words-owlqn2-distsim-pref6-inf2.tagger'\n",
    "jar = 'lib/stanford-postagger.jar'\n",
    "\n",
    "fspost = StanfordPOSTagger(model, path_to_jar=jar)  # Load Tagger Model\n",
    "fspost._SEPARATOR = '|'  # Set separator for proper tuple formatting (word, tag)\n",
    "\n",
    "def set_java_path(file_path):\n",
    "    \"\"\"\n",
    "    Function for setting java path to make Stanford POS Tagger work. Makes use of the 'os' library. Input \"\" to use\n",
    "    default java path, otherwise set the location.\n",
    "    Args:\n",
    "        file_path (str): The java file path / location.\n",
    "    \"\"\"\n",
    "    if file_path == \"\":\n",
    "        java_path = \"C:/Program Files/Java/jdk1.8.0_111/bin/java.exe\"\n",
    "        print(\"Java path set by default\")\n",
    "    else:\n",
    "        java_path = file_path\n",
    "        print(\"Java path set from given\")\n",
    "    os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "def tag_string(sentence):\n",
    "    \"\"\"\n",
    "    Function for tagging a sentence/string. Output is a (word, pos) tuple. To output a POS-only string, enclose this\n",
    "    function with 'format_pos' function. Ex. fspost.format_pos(fspost.tag_string('this is a string')). Same goes for\n",
    "    Stanford's word|tag notation, use 'format_stanford' function.\n",
    "    Args:\n",
    "        sentence (str): The string to be tagged.\n",
    "    Returns:\n",
    "        tagged_string: a list of string tokens containing POS labeled (word, pos) tuples.\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()  # Tokenize Sentence by whitespaces\n",
    "    # print(tokens)\n",
    "    tagged_string = fspost.tag(tokens)\n",
    "    return tagged_string\n",
    "\n",
    "def tag_string_list(sentence_list):\n",
    "    \"\"\"\n",
    "    Function for tagging a list of sentences. Output is a list of (word, pos) tuple. To output a POS-only string,\n",
    "    enclose the elements in this function with 'format_pos' function. Same goes for Stanford's word|tag notation, use\n",
    "    'format_stanford' function.\n",
    "    Args:\n",
    "        sentence_list (list): The list of strings to be tagged.\n",
    "    Returns:\n",
    "        tagged_list: a list of strings containing POS labelled (word, pos) tuples.\n",
    "    \"\"\"\n",
    "    progress_ctr = 0\n",
    "    tagged_list = []  # Initialize an empty list\n",
    "    for sentence in sentence_list:\n",
    "        tagged_tuple = tag_string(sentence)  # Tag each sentence in the list\n",
    "        tagged_list.append(tagged_tuple)  # Insert tagged sentence in the new list\n",
    "        progress_ctr += 1\n",
    "        print(progress_ctr, \"/\", len(sentence_list))  # Progress Counter\n",
    "    return tagged_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212e2ad",
   "metadata": {},
   "source": [
    "[REQUIRED] Set JDK Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b94167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOWS\n",
    "# set_java_path(\"C:/Program Files/Java/jdk-19/bin/java.exe\")\n",
    "\n",
    "# LINUX\n",
    "set_java_path(\"/usr/lib/jvm/java-11-openjdk-amd64/bin/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type C or c to cancel loop\n",
    "while True:\n",
    "    input_sentence_mgnn = input(\"Enter sample sentence: \")\n",
    "    \n",
    "    if input_sentence_mgnn == \"c\" or input_sentence_mgnn == \"C\":\n",
    "        break\n",
    "        \n",
    "    print(tag_string(input_sentence_mgnn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25f46f",
   "metadata": {},
   "source": [
    "## Get sample sentence from FilWordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf8f173",
   "metadata": {},
   "source": [
    "Import FilWordNet Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "filword_corpus = pd.read_csv(\"processed_corpus_oct_2022.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be099d0",
   "metadata": {},
   "source": [
    "Generate random string from FilWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ea3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the sentences with the source type online_forums, social_media, news_sites\n",
    "raw_sentences = filword_corpus[filword_corpus.source_type.isin(['online_forums', 'social_media', 'news_sites'])]\n",
    "\n",
    "#Drops the rows with a substring XX_...\n",
    "raw_sentences = raw_sentences.loc[~raw_sentences['text'].str.contains('XX_\\w{1,}')]\n",
    "#Drops the rows with a special character not included in ASCII dec 32-126\n",
    "sentences = raw_sentences.loc[~raw_sentences['text'].str.contains('[^\\x20-\\x7E]')]\n",
    "\n",
    "#Resets the index to start at 0. Since we removed some rows from the original data,\n",
    "#resetting the index must be performed\n",
    "sentences = sentences.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55807252",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "randInd = random.randrange(len(sentences))\n",
    "filword_randtext = sentences.text[randInd]\n",
    "\n",
    "print(filword_randtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9689d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make example sentence\n",
    "sentence = Sentence(filword_randtext)\n",
    "#AAAAAAAAA\n",
    "# Hello world!\n",
    "# predict NER tags\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print sentence\n",
    "print(sentence)\n",
    "\n",
    "# print predicted NER spans\n",
    "print('The following NER tags are found:')\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('pos'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683b590",
   "metadata": {},
   "source": [
    "## Combined POS Tagger - Combination 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b5d57",
   "metadata": {},
   "source": [
    "### Language Identification then Monolingual Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3c793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37982c1c",
   "metadata": {},
   "source": [
    "## ENGPOSTs Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afe353",
   "metadata": {},
   "source": [
    "### spaCy Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f79eda",
   "metadata": {},
   "source": [
    "Import spaCy and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc3858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d06aa3",
   "metadata": {},
   "source": [
    "Generate POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6fa3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_spacy(sentence):\n",
    "    \n",
    "    doc = spacy_nlp(sentence)\n",
    "    \n",
    "    for token in doc:\n",
    "        print(token, \": \", token.pos_, \": \", spacy.explain(token.pos_))\n",
    "\n",
    "print_spacy(filword_randtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53130f",
   "metadata": {},
   "source": [
    "### NLTK Testing (default ENGPOST, with FW tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55fac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# [IMPORTANT] if this is your first time running this Python Notebook, run this:\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = word_tokenize(filword_randtext)\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0419175",
   "metadata": {},
   "source": [
    "## FILPOSTs Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a88f445",
   "metadata": {},
   "source": [
    "### LSTM Based Filipino POS Tagger (Cruz, 2020)  ***unfinished***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as datautils\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.utils import predict, normalize, produce_vocab, proc_set, init_weights, accuracy\n",
    "from utils.model import LSTMTagger\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--do_train', action='store_true', help='Train a part of speech tagger.')\n",
    "    parser.add_argument('--do_predict', action='store_true', help='Use a trained model to predict parts of speech.')\n",
    "    parser.add_argument('--seed', type=int, default=1234, help='Random seed.')\n",
    "    parser.add_argument('--checkpoint', type=str, default='checkpoint', help='Location to save model.')\n",
    "    parser.add_argument('--overwrite_save_directory', action='store_true', help='Overwrite the save directory if it exists.')\n",
    "\n",
    "    parser.add_argument('--train_data', type=str, help='Training text dataset.')\n",
    "    parser.add_argument('--evaluation_data', type=str, help='Evaluation text dataset.')\n",
    "    parser.add_argument('--train_tags', type=str, help='Training tags dataset.')\n",
    "    parser.add_argument('--evaluation_tags', type=str, help='Evaluation tags dataset.')\n",
    "    parser.add_argument('--no_cuda', action='store_true', help='Do not use a GPU.')\n",
    "    \n",
    "    parser.add_argument('--embedding_dim', type=int, default=300, help='Embedding dimension.')\n",
    "    parser.add_argument('--num_layers', type=int, default=1, help='Number of recurrent layers.')\n",
    "    parser.add_argument('--bidirectional', action='store_true', help='Use a bidirectional RNN.')\n",
    "    parser.add_argument('--hidden_dim', type=int, default=512, help='Hidden dimension.')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5, help='Dropout probability.')\n",
    "    parser.add_argument('--recur_dropout', type=float, default=0.1, help='Recurrent dropout probability.')\n",
    "    parser.add_argument('--min_freq', type=int, default=1, help='Minimum frequency of words to be added to vocabulary.')\n",
    "    parser.add_argument('--msl', type=int, default=128, help='Maximum sequence length of text.')\n",
    "    parser.add_argument('--bs', type=int, default=128, help='Batch size.')\n",
    "    parser.add_argument('--learning_rate', type=float, default=3e-4, help='Learning rate.')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay.')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs to train.')\n",
    "    parser.add_argument('--sentence', type=str, default='Hello', help='Sentence to predict')\n",
    "    '''\n",
    "    \n",
    "    # args = parser.parse_args()\n",
    "    torch.manual_seed(args.seed);\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    '''\n",
    "    if args.do_train:\n",
    "        # Load Dataset\n",
    "        print(\"Loading dataset\")\n",
    "        with open(args.train_data, 'r') as f:\n",
    "            train_words = [line.strip() for line in f]\n",
    "        with open(args.evaluation_data, 'r') as f:\n",
    "            test_words = [line.strip() for line in f]\n",
    "        with open(args.train_tags, 'r') as f:\n",
    "            train_tags = [line.strip() for line in f]\n",
    "        with open(args.evaluation_tags, 'r') as f:\n",
    "            test_tags = [line.strip() for line in f]\n",
    "\n",
    "        # Normalize text\n",
    "        print(\"Normalizing text and producing vocabularies.\")\n",
    "        train_words = [normalize(line) for line in train_words]\n",
    "        test_words = [normalize(line) for line in test_words]\n",
    "\n",
    "        # Produce vocabularies\n",
    "        word_vocab, idx2word, word2idx = produce_vocab(train_words, min_freq=args.min_freq)\n",
    "        tags_vocab, idx2tag, tag2idx  = produce_vocab(train_tags, min_freq=args.min_freq)\n",
    "        print(\"Training word vocabulary has {:,} unique tokens.\".format(len(word_vocab)))\n",
    "        print(\"Training tags vocabulary has {:,} unique tokens.\".format(len(tags_vocab)))\n",
    "\n",
    "        # Produce sets\n",
    "        X_train = proc_set(train_words, word2idx, word_vocab, msl=args.msl)\n",
    "        y_train = proc_set(train_tags , tag2idx,  tags_vocab,  msl=args.msl)\n",
    "        X_test = proc_set(test_words, word2idx, word_vocab, msl=args.msl)\n",
    "        y_test = proc_set(test_tags , tag2idx,  tags_vocab,  msl=args.msl)\n",
    "\n",
    "        # Convert to tensors\n",
    "        X_train, y_train = torch.LongTensor(X_train), torch.LongTensor(y_train)\n",
    "        X_test, y_test = torch.LongTensor(X_test), torch.LongTensor(y_test)\n",
    "\n",
    "        # Produce dataloaders\n",
    "        train_set = datautils.TensorDataset(X_train, y_train)\n",
    "        test_set = datautils.TensorDataset(X_test, y_test)\n",
    "        train_sampler = datautils.RandomSampler(train_set)\n",
    "        train_loader = datautils.DataLoader(train_set, sampler=train_sampler, batch_size=args.bs)\n",
    "        test_loader = datautils.DataLoader(test_set, shuffle=False, batch_size=args.bs)\n",
    "\n",
    "        print(\"Training batches: {}\\nEvaluation batches: {}\".format(len(train_loader), len(test_loader)))\n",
    "\n",
    "        # Training setup\n",
    "        model = LSTMTagger(word_vocab_sz=len(word_vocab), \n",
    "                           tag_vocab_sz=len(tags_vocab), \n",
    "                           embedding_dim=args.embedding_dim, \n",
    "                           hidden_dim=args.hidden_dim, \n",
    "                           dropout=args.dropout,\n",
    "                           num_layers=args.num_layers,\n",
    "                           recur_dropout=args.recur_dropout,\n",
    "                           bidirectional=args.bidirectional).to(device)\n",
    "        model.apply(init_weights)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=tag2idx['<pad>'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "        print(\"Model has {:,} trainable parameters.\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))   \n",
    "\n",
    "        # Training\n",
    "        for e in range(1, args.epochs + 1):\n",
    "            model.train()\n",
    "            train_loss, train_acc = 0, 0\n",
    "            for x, y in tqdm(train_loader):\n",
    "                x, y = x.transpose(1, 0).to(device), y.transpose(1, 0).to(device)\n",
    "                out = model(x)\n",
    "                loss = criterion(out.flatten(0, 1), y.flatten(0))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                train_acc += accuracy(out, y, tag2idx)\n",
    "            train_loss /= len(train_loader)\n",
    "            train_acc /= len(train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            test_loss, test_acc = 0, 0\n",
    "            for x, y in tqdm(test_loader):\n",
    "                with torch.no_grad():\n",
    "                    x, y = x.transpose(1, 0).to(device), y.transpose(1, 0).to(device)\n",
    "                    out = model(x)\n",
    "                    loss = criterion(out.flatten(0, 1), y.flatten(0))\n",
    "                test_loss += loss.item()\n",
    "                test_acc += accuracy(out, y, tag2idx)\n",
    "            test_loss /= len(test_loader)\n",
    "            test_acc /= len(test_loader)\n",
    "\n",
    "            print(\"Epoch {:4} | Train Loss {:.4f} | Train Acc {:.2f}% | Test Loss {:.4f} | Test Acc {:.2f}%\".format(e, train_loss, train_acc, test_loss, test_acc))  \n",
    "        \n",
    "        # Save model\n",
    "        if args.overwrite_save_directory:\n",
    "            if os.path.exists(args.checkpoint): os.system('rm -r '+ args.checkpoint + '/')\n",
    "\n",
    "        print('Saving model and vocabularies.')\n",
    "        os.mkdir(args.checkpoint)\n",
    "        with open(args.checkpoint + '/model.bin', 'wb') as f:\n",
    "            torch.save(model.state_dict(), f)\n",
    "        with open(args.checkpoint + '/settings.bin', 'wb') as f:\n",
    "            torch.save([word_vocab, word2idx, idx2word, tags_vocab, tag2idx, idx2tag, args.msl, \n",
    "                        args.embedding_dim, args.hidden_dim, args.dropout, args.bidirectional, \n",
    "                        args.num_layers, args.recur_dropout], f)\n",
    "    '''\n",
    "    #if args.do_predict:\n",
    "        # Load the vocabularies\n",
    "    with open('checkpoint/settings.bin', 'rb') as f:\n",
    "        word_vocab, word2idx, idx2word, tags_vocab, tag2idx, idx2tag, msl, embedding_dim, hidden_dim, dropout, bidirectional, num_layers, recur_dropout = torch.load(f)\n",
    "\n",
    "        # Produce a blank model\n",
    "    model = LSTMTagger(word_vocab_sz=len(word_vocab), \n",
    "                        tag_vocab_sz=len(tags_vocab), \n",
    "                        embedding_dim=embedding_dim, \n",
    "                        hidden_dim=hidden_dim, \n",
    "                        dropout=dropout,\n",
    "                        num_layers=num_layers,\n",
    "                        recur_dropout=recur_dropout,\n",
    "                        bidirectional=bidirectional)\n",
    "\n",
    "    # Load checkpoints and put the model in eval mode\n",
    "    with open('checkpoint/model.bin', 'rb') as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "    model = model.cpu()\n",
    "    model.eval();\n",
    "\n",
    "    preds = predict(args.sentence, word2idx, idx2tag, word_vocab, msl, model)\n",
    "    print(preds)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
